{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11729309,"sourceType":"datasetVersion","datasetId":7131182}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom datetime import datetime\n\nfrom transformers import XGLMTokenizer, XGLMForCausalLM,AutoTokenizer,AutoModelForCausalLM, Trainer, TrainingArguments\nfrom transformers import DataCollatorForLanguageModeling\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import load_dataset,Dataset\n# from datasets import \nfrom tqdm import tqdm\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nimport random\nimport wandb\n\nwandb.login(key=\"8b274369bdaf41e5e92db13dc11653a1e7a97fe6\")\nwandb.init(project=\"nlp-onet-finetuning\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_reasoning_dataset(df):\n    # Format each example as an instruction with reasoning\n    formatted_data = []\n    for _, row in df.iterrows():\n        # Format for Thai reasoning problems\n        text = row['prompt']+row['reasoning']\n        formatted_data.append({\"text\": text,\n                               \"prompt\":row['prompt']\n                            })\n    return Dataset.from_pandas(pd.DataFrame(formatted_data))\n    \ndef tokenize(example):\n    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=1136)\n\ndef getLabels(example):\n    prompt_ids = tokenizer(example['prompt'], return_tensors=\"pt\").input_ids\n    labels = example['input_ids'].copy()\n    labels[:len(prompt_ids)] = [-100]*len(prompt_ids)\n    return {'labels':labels}\n    \ndef genPrompt(data,task=\"\\nกรุณาอธิบายเหตุผลอย่างเป็นขั้นตอนและระบุคำตอบที่ถูกต้องที่ท้ายคำอธิบาย (ในรูปแบบ: คำตอบคือ: b) ans: \",random_order=False):\n    # Set the seed for reproducibility\n    # random.seed(42)  # Use any integer you like\n    \n    choices = [\n        ('A', data['choice1']),\n        ('B', data['choice2']),\n        ('C', data['choice3']),\n        ('D', data['choice4']),\n        ('E', data['choice5']),\n    ]\n    # choices = [\n    #     ('\\n', data['choice1']),\n    #     ('\\n', data['choice2']),\n    #     ('\\n', data['choice3']),\n    #     ('\\n', data['choice4']),\n    #     ('\\n', data['choice5']),\n    # ]\n    \n    # Shuffle the choices\n    if random_order :\n        random.shuffle(choices)\n    \n    # Build the prompt\n    prompt = data['question'] + \"\\nตัวเลือก:\\n\"\n    for label, choice in choices:\n        prompt += f\"{label}. {choice}\\n\"\n        # prompt += f\"{label}|{choice}|\"\n        \n    \n    prompt += task\n    return prompt\n    # return  data['question'] + \"\\nตัวเลือก:\\n\" + \"A. \"+data['choice1'] + \"\\nB. \"+data['choice2']+\"\\nC. \"+data['choice3']+\"\\nD. \"+data['choice4']+\"\\nE. \"+data['choice5']+ task\n\ndef genPromptV2(data):\n    return genPrompt(data,\"\")+\"\\nYour task is to answer the given multiple choice mathamatical question by thinking step by step.\\n You must add <think> when the thinking started and </think> when the thinking has ended, then answer the choice in which you think is correct in this example format (answer: A.). To answer \"\n\ndef genPromptNoChoice(data):\n    return data['question'] + \"\\nans:\"\n\ndef genAns(input_text,max_new_tokens = 200):\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    tokenizer.pad_token = tokenizer.eos_token  # Make sure they're properly linke        d\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n    \n    generated_ids = model.generate(\n        input_ids.input_ids,\n        attention_mask=input_ids.attention_mask,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,  # Disable sampling for more deterministic outputs\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        # repetition_penalty=1.1,  # Mild repetition penalty\n        num_beams=3,     # Use beam search for more coherent outputs\n        early_stopping=True\n    )\n    \n    # Decode the generated ids to text\n    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n    return generated_text\n\ndef genSamples(df,n=20):\n    for i in range(n):\n        generated_text = genAns(genPrompt(df.iloc[i]))\n        print(generated_text) # 1.9B\n        print('correct ans:',df.iloc[i]['choice_ans'])\n        print(\"-\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"# model_name = \"facebook/xglm-564M\"\n# model_name = \"facebook/xglm-1.7B\"\nmodel_name = \"facebook/xglm-2.9B\"\n\nsafe_model_name = model_name.replace(\"/\", \"_\")\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\noutput_dir = safe_model_name + \"_\" + timestamp\ntokenizer = XGLMTokenizer.from_pretrained(model_name)\nmodel = XGLMForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,  # Use half precision to save memory\n    device_map=\"auto\"  # Automatically distribute model across available GPUs\n)\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=4,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    # bias=\"none\",\n    target_modules=[\"q_proj\", \"v_proj\",\"k_proj\",\"out_proj\"]  # Explicitly required\n)\n\nmodel = get_peft_model(model, peft_config)\nprint(model.print_trainable_parameters())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/project-nlp-onet-math/preped_data.csv\")\ndataset = prepare_reasoning_dataset(df)\ndataset = dataset.map(tokenize)\ndataset = dataset.map(getLabels)\nsplit_dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Before","metadata":{}},{"cell_type":"code","source":"genSamples(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_accumulation_steps=8,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    # greater_is_better=False,\n    learning_rate=5e-5,\n    num_train_epochs=50,\n    weight_decay=0.01,\n    report_to=\"wandb\",  # ใช้ WandB ในการติดตาม\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    fp16=True,  # ใช้ FP16 สำหรับการฝึก\n    label_names=[\"labels\"],\n    gradient_accumulation_steps=8  # สะสม gradient สำหรับหลายรอบ\n)\n\n# Create data collator for language modeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # We're not doing masked language modeling\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=split_dataset[\"train\"],\n    eval_dataset=split_dataset[\"test\"],\n    data_collator=data_collator,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start training\nprint(\"Starting training...\")\ntrainer.train()\n\n# Save the model\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# After","metadata":{}},{"cell_type":"code","source":"tokenizer = XGLMTokenizer.from_pretrained(output_dir)\nmodel = XGLMForCausalLM.from_pretrained(\n    output_dir,\n    torch_dtype=torch.float16,  # Use half precision to save memory\n    device_map=\"auto\"  # Automatically distribute model across available GPUs\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"genSamples(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}